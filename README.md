# ğŸš€ Cricklytics
## ğŸ“š Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Technologies Used](#technologies-used)
- [Setup and Installation](#setup-and-installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Data Sources](#data-sources)
- [Data Transformation Pipeline](#data-transformation-pipeline)
- [Future Enhancements](#future-enhancements)
- [License](#license)

## ğŸ” Overview

This project aims to provide insightful, interactive dashboards by collecting data from various online sources using web scraping, processing the data through several transformation steps, and visualizing the results in Power BI. The dashboard offers key metrics and analytics for data-driven decision-making.
![Dashboard](https://github.com/user-attachments/assets/30f200a6-71db-4d23-bbad-cae25082cd43)

## âœ¨ Features

- **Interactive Dashboards:** Utilizes Power BI to present dynamic and responsive visualizations. ğŸ“Š
- **Automated Web Scraping:** Extracts real-time or periodically updated data from targeted websites. ğŸŒ
- **Data Transformation:** Cleanses, transforms, and aggregates raw data to make it analysis-ready. ğŸ”„
- **Modular Architecture:** Easily extend or modify the scraping and transformation processes as needed. ğŸ› ï¸
- **Custom Reporting:** Offers tailored insights based on user-defined criteria and filters. ğŸ“ˆ

## ğŸ› ï¸ Technologies Used

- **Power BI:** For creating and hosting interactive dashboards.
- **Python (or other scripting language):** For web scraping and data transformation tasks.
- **Pandas / DataFrame libraries:** To handle and transform datasets.
- **BeautifulSoup / Selenium:** (or other web scraping tools) to extract data from web sources.
- **REST APIs:** Where applicable, to fetch data in a structured format.
- **SQL / NoSQL databases:** For data storage and retrieval (if applicable).

## âš™ï¸ Setup and Installation

### Prerequisites

- **Power BI Desktop:** [Download and install](https://powerbi.microsoft.com/desktop/) ğŸ’»
- **Python 3.x:** Ensure Python is installed on your system.
- Required Python libraries:
  - `pandas`
  - `requests`
  - `beautifulsoup4` or `selenium`
  - Additional libraries as specified in `requirements.txt`

### Installation Steps

  ## Clone the Repository

   ```bash
   git clone https://github.com/yourusername/datadashboardproject.git
   cd datadashboardproject
  ```

## Setup Python Environment

### Create a Virtual Environment (Optional but Recommended)
```bash
python -m venv env
source env/bin/activate  # On Windows: env\Scripts\activate
```

### Install Dependencies
```bash
pip install -r requirements.txt
```

## Configure Data Sources
Update the configuration file (`config.json` or similar) with your target URLs and API keys if needed.

## Run Data Scraping and Transformation Scripts
```bash
python run_pipeline.py
```

## Open Power BI Dashboard
- Open the provided Power BI file (`Dashboard.pbix`) in Power BI Desktop.
- Connect to the transformed data source (e.g., CSV file, database) if required.
- Refresh the dataset to load the latest data.

## ğŸ¯ Usage

### Data Collection
- Execute the scraping script to pull data from the predefined websites. ğŸŒ
- Verify the downloaded data in the `data/raw` directory.

### Data Transformation
- The transformation script cleans and aggregates the data.
- Processed data is saved in the `data/processed` directory. ğŸ”„

### Dashboard Interaction
- Use Power BI to interact with the dashboard.
- Apply filters, drill down into metrics, and generate reports as needed. ğŸ“Š

## ğŸ“ Project Structure
```
datadashboardproject/  
â”œâ”€â”€ Stage-2.pbix                # Power BI dashboard file (Stage 2)  
â”œâ”€â”€ Stage-3.pbix                # Power BI dashboard file (Stage 3)  
â”œâ”€â”€ t20_cric_1_power_query.pbix  # Power BI project for T20 cricket analysis  
â”œâ”€â”€ t20_csv_files.zip            # Compressed CSV files for T20 data  
â”œâ”€â”€ t20_json_files.zip           # Compressed JSON files for T20 data  
â”œâ”€â”€ web_scrapping_codes.zip      # Compressed web scraping scripts  
â”œâ”€â”€ t20_data_preprocessing.ipynb # Jupyter Notebook for data preprocessing  
â”œâ”€â”€ requirements.txt             # Python dependencies list  
â”œâ”€â”€ config.json                  # Configuration file for URLs, API keys, etc.  
â””â”€â”€ README.md                    # Project documentation  

```

## ğŸŒ Data Sources
- List the primary websites or APIs used for scraping.
- Mention any restrictions or limitations related to the data (e.g., scraping frequency, legal considerations).

## ğŸ”„ Data Transformation Pipeline
- **Extraction:** Data is scraped from multiple sources using designated tools and libraries.
- **Transformation:** Data is cleaned, normalized, and enriched to ensure consistency.
- **Loading:** The transformed data is exported in formats suitable for Power BI (e.g., CSV, SQL database).

## ğŸš€ Future Enhancements
- **Real-Time Data Updates:** Implement a scheduler to automate the scraping and dashboard refresh. â±ï¸
- **Advanced Analytics:** Incorporate machine learning models for predictive analytics. ğŸ¤–
- **Enhanced Visualization:** Expand the dashboard with additional KPIs and interactive elements. ğŸ“Š
- **User Authentication:** Secure access to the dashboard for different user roles. ğŸ”’
